{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques.\n",
    "\n",
    "SOme help from [https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb]\n",
    "\n",
    "### Requires\n",
    "Data sets should be generated by running by this notebook\n",
    "http://localhost:8888/notebooks/sandbox/ipynotebook/UD730/1_notmnist-dataprep.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`; and then reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784), (200000,)\n",
      "Validation set (10000, 784) (10000,)\n",
      "Test set (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  #print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  #print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  #print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "# reshape the image parts 28x28 -> 784\n",
    "(n,width,height) = train_dataset.shape\n",
    "train_dataset =  np.reshape(train_dataset,(n,width*height))[0:n]\n",
    "(n,width,height) = valid_dataset.shape\n",
    "valid_dataset =  np.reshape(valid_dataset,(n,width*height))[0:n]\n",
    "(n,width,height) = test_dataset.shape\n",
    "test_dataset =  np.reshape(test_dataset,(n,width*height))[0:n]\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "trainTF_dataset, trainTF_labels = reformat(train_dataset, train_labels)\n",
    "validTF_dataset, validTF_labels = reformat(valid_dataset, valid_labels)\n",
    "testTF_dataset, testTF_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set %s, %s'  % ( train_dataset.shape, train_labels.shape))\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784), (200000,)\n"
     ]
    }
   ],
   "source": [
    "# this part at the top truncate the data to make it easier to manipulate\n",
    "\n",
    "train_subset = 200000\n",
    "idx = np.random.randint(train_labels.shape[0], size=train_subset)\n",
    "train_dataset = train_dataset[idx, :]\n",
    "train_labels = train_labels[idx]\n",
    "\n",
    "trainTF_dataset, trainTF_labels = reformat(train_dataset, train_labels)\n",
    "validTF_dataset, validTF_labels = reformat(valid_dataset, valid_labels)\n",
    "testTF_dataset, testTF_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set %s, %s'  % ( train_dataset.shape, train_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will be used for scoring normal vectors\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(predictions == labels) / predictions.shape[0])\n",
    "\n",
    "# this will be used for scoring one-hot\n",
    "def accuracyTF(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / labels.shape[0])\n",
    "\n",
    "# placeholders for our Tensor flow input and output\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32) # DROP OUT here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Logistic Network with L2 Regularization\n",
    "\n",
    "Copy from Lesson 1(part2), and add L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 83.51%\t Validation: 81.97%\t Test: 88.93%\n",
      "Elapsed Time: 1233.30\n"
     ]
    }
   ],
   "source": [
    "def SKLogistic():\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_dataset, train_labels)\n",
    "\n",
    "    train_predicted = clf.predict(train_dataset)\n",
    "    valid_predicted = clf.predict(valid_dataset)\n",
    "    test_predicted = clf.predict(test_dataset)\n",
    "    return (accuracy(train_predicted, train_labels), \n",
    "            accuracy(valid_predicted, valid_labels), \n",
    "            accuracy(test_predicted, test_labels))\n",
    "\n",
    "start = time.time()\n",
    "sk_trn_score, sk_vld_score, sk_tst_score = SKLogistic()\n",
    "print(\"Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (sk_trn_score, sk_vld_score, sk_tst_score))\n",
    "print(\"Elapsed Time: %.2f\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Factor 0.000: TFLog Training: 77.81%\t Validation: 77.19%\t Test: 84.63%\n",
      "Elapsed Time: 44.70\n",
      "Reg Factor 0.001: TFLog Training: 81.48%\t Validation: 80.36%\t Test: 87.78%\n",
      "Elapsed Time: 44.80\n",
      "Reg Factor 0.005: TFLog Training: 83.77%\t Validation: 82.93%\t Test: 89.71%\n",
      "Elapsed Time: 44.78\n",
      "Reg Factor 0.010: TFLog Training: 83.56%\t Validation: 82.88%\t Test: 89.66%\n",
      "Elapsed Time: 44.48\n"
     ]
    }
   ],
   "source": [
    "# the TF way - stuff must be one-hoted\n",
    "\n",
    "def TFLogistic(batch_size=128, training_epochs=50, display_step=100, learning_rate=0.01, reg_factor=0.01):\n",
    "\n",
    "    W = tf.Variable(tf.truncated_normal([784, 10]), name=\"weights\")\n",
    "    b = tf.Variable(tf.truncated_normal([10]), name=\"biases\")\n",
    "\n",
    "    logits = tf.matmul(X,W) + b\n",
    "    predict = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "    # for L2 regularization we add something here \n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) +\n",
    "            reg_factor * tf.nn.l2_loss(W))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0.\n",
    "            total_batch = int(trainTF_dataset.shape[0]/batch_size)\n",
    "            for step in range(total_batch):\n",
    "                offset = (step * batch_size) % (trainTF_labels.shape[0] - batch_size)\n",
    "                batch_xs = trainTF_dataset[offset:(offset + batch_size), :]\n",
    "                batch_ys = trainTF_labels[offset:(offset + batch_size)]\n",
    "                _, l, p = sess.run([optimizer, loss, predict], feed_dict={X: batch_xs, y: batch_ys})\n",
    "                avg_loss += l / total_batch # add this rounds portion of the loss avg\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: %04d, loss = %.9f\" % ( (epoch+1), avg_loss))\n",
    "            \n",
    "        trainTF_predicted = sess.run([predict], feed_dict={X: trainTF_dataset})\n",
    "        validTF_predicted = sess.run([predict], feed_dict={X: validTF_dataset})\n",
    "        testTF_predicted = sess.run([predict], feed_dict={X: testTF_dataset})\n",
    "\n",
    "    return (accuracyTF(trainTF_predicted[0], trainTF_labels), \n",
    "            accuracyTF(validTF_predicted[0], validTF_labels), \n",
    "            accuracyTF(testTF_predicted[0], testTF_labels))\n",
    "\n",
    "\n",
    "for reg_factor in (0.0, 0.001, 0.005, 0.01):\n",
    "    start = time.time()\n",
    "    lg_trn_score, lg_vld_score, lg_tst_score = TFLogistic(reg_factor=reg_factor)\n",
    "    print(\"Reg Factor %5.3f: TFLog Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (reg_factor,lg_trn_score, lg_vld_score, lg_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Netowk with L2 Regularization\n",
    "\n",
    "Brining in some of what I learned here [http://localhost:8888/notebooks/sandbox/ipynotebook/tensorflow/NN-MNISTdigits.ipynb] assuming I am running locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Factor 0.000: TFNet Training: 89.27%\t Validation: 83.97%\t Test: 90.50%\n",
      "Elapsed Time: 712.97\n",
      "Reg Factor 0.001: TFNet Training: 82.88%\t Validation: 82.15%\t Test: 89.27%\n",
      "Elapsed Time: 713.24\n",
      "Reg Factor 0.005: TFNet Training: 81.68%\t Validation: 81.33%\t Test: 88.25%\n",
      "Elapsed Time: 727.47\n",
      "Reg Factor 0.010: TFNet Training: 79.94%\t Validation: 79.41%\t Test: 86.42%\n",
      "Elapsed Time: 743.82\n"
     ]
    }
   ],
   "source": [
    "def TFNet(n_hidden1 = 1024, batch_size=128, training_epochs=50, display_step=100, learning_rate=0.01, reg_factor=0.01):\n",
    "    n_input = trainTF_dataset.shape[1]\n",
    "    n_classes = trainTF_labels.shape[1]\n",
    "\n",
    "    # model weights\n",
    "    hl1   = {'weights':tf.Variable(tf.random_normal([n_input,n_hidden1])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_hidden1]))}\n",
    "    outer = {'weights':tf.Variable(tf.random_normal([n_hidden1,n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(X, hl1['weights']), hl1['biases']))\n",
    "    predict = tf.add(tf.matmul(layer1, outer['weights']), outer['biases'])\n",
    "\n",
    "    # with L2 regularization\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict)) +\n",
    "            reg_factor*tf.nn.l2_loss(hl1['weights']) +\n",
    "            reg_factor*tf.nn.l2_loss(outer['weights']))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    n_trainingsize = trainTF_labels.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0.\n",
    "            total_batch = int(trainTF_dataset.shape[0]/batch_size)\n",
    "            for step in range(total_batch):\n",
    "                offset = (step * batch_size) % (n_trainingsize - batch_size)\n",
    "                batch_xs = trainTF_dataset[offset:(offset + batch_size), :]\n",
    "                batch_ys = trainTF_labels[offset:(offset + batch_size)]\n",
    "                _, l = sess.run([optimizer, loss], feed_dict={X: batch_xs, y: batch_ys})\n",
    "                avg_loss += l / total_batch # add this rounds portion of the loss avg\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: %04d, loss = %.9f\" % ( (epoch+1), avg_loss))\n",
    "    \n",
    "        trainTF_predicted = sess.run([predict], feed_dict={X: trainTF_dataset})\n",
    "        validTF_predicted = sess.run([predict], feed_dict={X: validTF_dataset})\n",
    "        testTF_predicted = sess.run([predict], feed_dict={X: testTF_dataset})\n",
    "\n",
    "    return (accuracyTF(trainTF_predicted[0], trainTF_labels), \n",
    "            accuracyTF(validTF_predicted[0], validTF_labels), \n",
    "            accuracyTF(testTF_predicted[0], testTF_labels))\n",
    "\n",
    "\n",
    "for reg_factor in (0.0, 0.001, 0.005, 0.01):\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNet(reg_factor=reg_factor)\n",
    "    print(\"Reg Factor %5.3f: TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (reg_factor,nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size: 200000\n",
      "SKLog Training: 83.56%\t Validation: 82.88%\t Test: 89.66%\n",
      "TFLog Training: 79.94%\t Validation: 79.41%\t Test: 86.42%\n",
      "TFNet Training: 83.51%\t Validation: 81.97%\t Test: 88.93%\n"
     ]
    }
   ],
   "source": [
    "#summarize all scores\n",
    "print(\"Train Data Size: %d\" % train_dataset.shape[0])\n",
    "print(\"SKLog Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (lg_trn_score, lg_vld_score, lg_tst_score))\n",
    "print(\"TFLog Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "print(\"TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (sk_trn_score, sk_vld_score, sk_tst_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 64\n",
      "Size   100: TFNet Training: 6.00%\t Validation: 7.45%\t Test: 6.91%\n",
      "Elapsed Time: 0.78\n",
      "Size   500: TFNet Training: 90.60%\t Validation: 58.48%\t Test: 63.91%\n",
      "Elapsed Time: 2.23\n",
      "Size  1000: TFNet Training: 97.50%\t Validation: 75.49%\t Test: 82.49%\n",
      "Elapsed Time: 4.21\n",
      "Size  2000: TFNet Training: 93.15%\t Validation: 75.55%\t Test: 82.81%\n",
      "Elapsed Time: 7.77\n",
      "Size  3000: TFNet Training: 91.57%\t Validation: 76.42%\t Test: 83.59%\n",
      "Elapsed Time: 11.56\n",
      "Size  4000: TFNet Training: 88.70%\t Validation: 74.84%\t Test: 82.63%\n",
      "Elapsed Time: 15.70\n",
      "Size  5000: TFNet Training: 90.70%\t Validation: 79.07%\t Test: 85.98%\n",
      "Elapsed Time: 19.52\n",
      "\n",
      "batch_size 128\n",
      "Size   100: TFNet Training: 11.00%\t Validation: 8.15%\t Test: 7.57%\n",
      "Elapsed Time: 1.21\n",
      "Size   500: TFNet Training: 89.60%\t Validation: 55.22%\t Test: 60.86%\n",
      "Elapsed Time: 2.70\n",
      "Size  1000: TFNet Training: 97.10%\t Validation: 75.48%\t Test: 83.28%\n",
      "Elapsed Time: 4.45\n",
      "Size  2000: TFNet Training: 92.10%\t Validation: 74.34%\t Test: 81.99%\n",
      "Elapsed Time: 8.51\n",
      "Size  3000: TFNet Training: 88.83%\t Validation: 74.25%\t Test: 81.23%\n",
      "Elapsed Time: 12.21\n",
      "Size  4000: TFNet Training: 89.85%\t Validation: 76.25%\t Test: 82.97%\n",
      "Elapsed Time: 16.04\n",
      "Size  5000: TFNet Training: 90.62%\t Validation: 77.45%\t Test: 84.47%\n",
      "Elapsed Time: 19.99\n",
      "\n",
      "batch size 1024\n",
      "Size  3000: TFNet Training: 88.17%\t Validation: 64.08%\t Test: 71.41%\n",
      "Elapsed Time: 6.63\n",
      "Size  4000: TFNet Training: 92.45%\t Validation: 65.97%\t Test: 73.82%\n",
      "Elapsed Time: 8.95\n",
      "Size  5000: TFNet Training: 96.50%\t Validation: 77.97%\t Test: 85.67%\n",
      "Elapsed Time: 11.15\n",
      "Size  6000: TFNet Training: 95.43%\t Validation: 78.90%\t Test: 86.69%\n",
      "Elapsed Time: 13.60\n",
      "Size  7000: TFNet Training: 89.07%\t Validation: 75.82%\t Test: 83.15%\n",
      "Elapsed Time: 16.05\n",
      "Size  8000: TFNet Training: 93.81%\t Validation: 78.41%\t Test: 86.03%\n",
      "Elapsed Time: 18.41\n",
      "Size  9000: TFNet Training: 95.72%\t Validation: 80.01%\t Test: 88.14%\n",
      "Elapsed Time: 20.88\n",
      "Size 10000: TFNet Training: 94.47%\t Validation: 79.35%\t Test: 86.64%\n",
      "Elapsed Time: 23.31\n"
     ]
    }
   ],
   "source": [
    "# set the original ones aside\n",
    "sTrainTF_dataset = trainTF_dataset\n",
    "sTrainTF_labels = trainTF_labels\n",
    "\n",
    "\n",
    "print(\"batch_size 64\")\n",
    "# better yet take a random selection\n",
    "for subset_size in (100, 500, 1000, 2000, 3000, 4000, 5000):\n",
    "    idx = np.random.randint(sTrainTF_labels.shape[0], size=subset_size)\n",
    "    trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "    trainTF_labels = sTrainTF_labels[idx,:]\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNet(batch_size=128, display_step=1000, reg_factor=0.001)\n",
    "    print(\"Size %5d: TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (subset_size, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "\n",
    "print(\"\")\n",
    "print(\"batch_size 128\")\n",
    "# better yet take a random selection\n",
    "for subset_size in (100, 500, 1000, 2000, 3000, 4000, 5000):\n",
    "    idx = np.random.randint(sTrainTF_labels.shape[0], size=subset_size)\n",
    "    trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "    trainTF_labels = sTrainTF_labels[idx,:]\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNet(batch_size=128, display_step=1000, reg_factor=0.001)\n",
    "    print(\"Size %5d: TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (subset_size, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "\n",
    "print(\"\")\n",
    "print(\"batch size 1024\")\n",
    "for subset_size in (3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000):\n",
    "    idx = np.random.randint(sTrainTF_labels.shape[0], size=subset_size)\n",
    "    trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "    trainTF_labels = sTrainTF_labels[idx,:]\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNet(batch_size=1024, display_step=1000, reg_factor=0.001)\n",
    "    print(\"Size %5d: TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (subset_size, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# restore the old values\n",
    "trainTF_dataset = sTrainTF_dataset\n",
    "trainTF_labels = sTrainTF_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Factor 0.000: TFNetDropout Training: 85.63%\t Validation: 83.98%\t Test: 90.91%\n",
      "Elapsed Time: 796.72\n",
      "Reg Factor 0.001: TFNetDropout Training: 81.98%\t Validation: 81.30%\t Test: 88.81%\n",
      "Elapsed Time: 794.94\n"
     ]
    }
   ],
   "source": [
    "#  hmmm... how to introduce dropout... I guess google :-)\n",
    "# for now it just does relu instead of sigmoid\n",
    "def TFNetDropout(n_hidden1 = 1024, batch_size=128, training_epochs=50, \n",
    "                 display_step=100, learning_rate=0.01, reg_factor=0.01, keep_factor=.5):\n",
    "    n_input = trainTF_dataset.shape[1]\n",
    "    n_classes = trainTF_labels.shape[1]\n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # model weights\n",
    "    hl1   = {'weights':tf.Variable(tf.random_normal([n_input,n_hidden1])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_hidden1]))}\n",
    "    outer = {'weights':tf.Variable(tf.random_normal([n_hidden1,n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(X, hl1['weights']), hl1['biases']))\n",
    "    drop_out = tf.nn.dropout(layer1, keep_prob ) # apply drop-out to the hidden layer\n",
    "    predict = tf.add(tf.matmul(drop_out, outer['weights']), outer['biases'])\n",
    "\n",
    "    # with L2 regularization\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict)) +\n",
    "            reg_factor*tf.nn.l2_loss(hl1['weights']) +\n",
    "            reg_factor*tf.nn.l2_loss(outer['weights']))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    n_trainingsize = trainTF_labels.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0.\n",
    "            total_batch = int(trainTF_dataset.shape[0]/batch_size)\n",
    "            for step in range(total_batch):\n",
    "                offset = (step * batch_size) % (n_trainingsize - batch_size)\n",
    "                batch_xs = trainTF_dataset[offset:(offset + batch_size), :]\n",
    "                batch_ys = trainTF_labels[offset:(offset + batch_size)]\n",
    "                _, l = sess.run([optimizer, loss], feed_dict={X: batch_xs, y: batch_ys, keep_prob: keep_factor})\n",
    "                avg_loss += l / total_batch # add this rounds portion of the loss avg\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: %04d, loss = %.9f\" % ( (epoch+1), avg_loss))\n",
    "    \n",
    "        trainTF_predicted = sess.run([predict], feed_dict={X: trainTF_dataset, keep_prob:1.0})\n",
    "        validTF_predicted = sess.run([predict], feed_dict={X: validTF_dataset, keep_prob:1.0})\n",
    "        testTF_predicted = sess.run([predict], feed_dict={X: testTF_dataset, keep_prob:1.0})\n",
    "\n",
    "    return (accuracyTF(trainTF_predicted[0], trainTF_labels), \n",
    "            accuracyTF(validTF_predicted[0], validTF_labels), \n",
    "            accuracyTF(testTF_predicted[0], testTF_labels))\n",
    "\n",
    "    \n",
    "for reg_factor in (0.0, 0.001):\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNetDropout(reg_factor=reg_factor)\n",
    "    print(\"Reg Factor %5.3f: TFNetDropout Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (reg_factor, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2000\n",
      "Epoch: 0100, loss = 2.326414466\n",
      "keep 0.50: TFNetDropout Training: 94.20%\t Validation: 78.36%\t Test: 85.63%\n",
      "Elapsed Time: 18.08\n",
      "Epoch: 0100, loss = 1.642942373\n",
      "keep 0.70: TFNetDropout Training: 93.90%\t Validation: 77.89%\t Test: 85.45%\n",
      "Elapsed Time: 17.15\n",
      "Epoch: 0100, loss = 1.423363598\n",
      "keep 0.90: TFNetDropout Training: 92.90%\t Validation: 76.00%\t Test: 82.58%\n",
      "Elapsed Time: 17.10\n",
      "Epoch: 0100, loss = 1.215106161\n",
      "keep 1.00: TFNetDropout Training: 91.65%\t Validation: 74.83%\t Test: 82.15%\n",
      "Elapsed Time: 17.78\n"
     ]
    }
   ],
   "source": [
    "# run some extreme overfitting;\n",
    "# set the original ones aside\n",
    "sTrainTF_dataset = trainTF_dataset\n",
    "sTrainTF_labels = trainTF_labels\n",
    "\n",
    "print(\"batch_size 2000\")\n",
    "idx = np.random.randint(sTrainTF_labels.shape[0], size=2000)\n",
    "trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "trainTF_labels = sTrainTF_labels[idx,:]\n",
    "\n",
    "# better yet take a random selection\n",
    "for keep_factor in (.5, .7, .9, 1.):\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNetDropout(training_epochs=100, reg_factor=0.001, keep_factor=keep_factor)\n",
    "    print(\"keep %.2f: TFNetDropout Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (keep_factor, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "#nn_trn_score, nn_vld_score, nn_tst_score = TFNetDropout(reg_factor=0.001, keep_factor=0.5)\n",
    "#print(\"keep %2f: TFNetDropout Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (keep_factor, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "\n",
    "\n",
    "# restore the old values\n",
    "trainTF_dataset = sTrainTF_dataset\n",
    "trainTF_labels = sTrainTF_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assumes train,test,valid dataset/labels as  trainTF_dataset are all defined\n",
    "\n",
    "def TFNetBest(n_hidden1 = 1024, n_hidden2 = 512, training_epochs=100, batch_size = 128,\n",
    "                 display_step=200, learning_rate=0.001, reg_factor=0.001, keep_factor=.5):\n",
    "    n_input = trainTF_dataset.shape[1]\n",
    "    n_classes = trainTF_labels.shape[1]\n",
    "    \n",
    "    # model weights\n",
    "    hl1   = {'weights':tf.Variable(tf.random_normal([n_input,n_hidden1])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_hidden1]))}\n",
    "    hl2   = {'weights':tf.Variable(tf.random_normal([n_hidden1,n_hidden2])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_hidden2]))}\n",
    "    outer = {'weights':tf.Variable(tf.random_normal([n_hidden2,n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))} \n",
    "    #global_step = tf.Variable(0) # count the number of steps taken\n",
    "    #learning_rate \n",
    "    \n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(X, hl1['weights']), hl1['biases']))\n",
    "    drop_out = tf.nn.dropout(layer1, keep_prob ) # apply drop-out to the hidden layer\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(drop_out, hl2['weights']), hl2['biases']))\n",
    "    predict = tf.add(tf.matmul(layer2, outer['weights']), outer['biases'])\n",
    "\n",
    "    # with L2 regularization\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict)) +\n",
    "            reg_factor*tf.nn.l2_loss(hl1['weights']) +\n",
    "            reg_factor*tf.nn.l2_loss(hl2['weights']) +\n",
    "            reg_factor*tf.nn.l2_loss(outer['weights']))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "    n_trainingsize = trainTF_labels.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0.\n",
    "            total_batch = int(trainTF_dataset.shape[0]/batch_size)\n",
    "            for step in range(total_batch):\n",
    "                offset = (step * batch_size) % (n_trainingsize - batch_size)\n",
    "                batch_xs = trainTF_dataset[offset:(offset + batch_size), :]\n",
    "                batch_ys = trainTF_labels[offset:(offset + batch_size)]\n",
    "                _, l = sess.run([optimizer, loss], feed_dict={X: batch_xs, y: batch_ys, keep_prob: keep_factor})\n",
    "                avg_loss += l / total_batch # add this rounds portion of the loss avg\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: %04d, loss = %.9f\" % ( (epoch+1), avg_loss))\n",
    "    \n",
    "        trainTF_predicted = sess.run([predict], feed_dict={X: trainTF_dataset, keep_prob:1.0})\n",
    "        validTF_predicted = sess.run([predict], feed_dict={X: validTF_dataset, keep_prob:1.0})\n",
    "        testTF_predicted = sess.run([predict], feed_dict={X: testTF_dataset, keep_prob:1.0})\n",
    "\n",
    "    return (accuracyTF(trainTF_predicted[0], trainTF_labels), \n",
    "            accuracyTF(validTF_predicted[0], validTF_labels), \n",
    "            accuracyTF(testTF_predicted[0], testTF_labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer2  128: TFNet/Best Training: 85.78%\t Validation: 85.00%\t Test: 91.68%\n",
      "Elapsed Time: 1777.21\n",
      "hidden layer2  512: TFNet/Best Training: 85.82%\t Validation: 85.13%\t Test: 91.88%\n",
      "Elapsed Time: 3046.73\n",
      "hidden layer2 1024: TFNet/Best Training: 85.88%\t Validation: 85.14%\t Test: 91.81%\n",
      "Elapsed Time: 4362.33\n"
     ]
    }
   ],
   "source": [
    "#grab a smaller set to test with\n",
    "\n",
    "#print(\"batch_size 2000\")\n",
    "#idx = np.random.randint(sTrainTF_labels.shape[0], size=2000)\n",
    "#trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "#trainTF_labels = sTrainTF_labels[idx,:]\n",
    "\n",
    "# better yet take a random selection\n",
    "for n_hidden_layer2 in (128, 512, 1024):\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNetBest(n_hidden2=n_hidden_layer2)\n",
    "    print(\"hidden layer2 %4d: TFNet/Best Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (n_hidden_layer2, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "#nn_trn_score, nn_vld_score, nn_tst_score = TFNetBest(reg_factor=0.001, keep_factor=0.5)\n",
    "#print(\"keep %2f: TFNetDropout Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (keep_factor, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "\n",
    "\n",
    "# restore the old values\n",
    "#trainTF_dataset = sTrainTF_dataset\n",
    "#trainTF_labels = sTrainTF_labels"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
