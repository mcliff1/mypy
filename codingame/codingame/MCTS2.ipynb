{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Sample from https://qiita.com/y_yoko/items/07b9e3e8d4a43c61d39f\n",
    "\n",
    "Solves the problem\n",
    "\n",
    "$$\n",
    "f(x) = (x_1 - a)^2 + (x_2 -b)^2+(x_3-c)^2+(x_4-d)^2\n",
    "$$\n",
    "\n",
    "which is minimized by picking $x = (a,b,c,d)$\n",
    "\n",
    "In our sample problem we can choose moves from $-4, ..., 4$ where $\\pm n \\mapsto x_n \\pm 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated best: [-2.0, 2.0, -5.0, -1.0]\n",
      "Terminal reached during a playout. #########\n",
      "Terminal reached during a playout. #########\n",
      "updated best: [0.0, 2.0, 1.0, 2.0]\n",
      "Terminal reached during a playout. #########\n",
      "Terminal reached during a playout. #########\n",
      "Terminal reached during a playout. #########\n",
      "Terminal reached during a playout. #########\n",
      "Terminal reached during a playout. #########\n",
      "Terminal reached during a playout. #########\n",
      "Terminal reached during a playout. #########\n",
      "Terminal reached during a playout. #########\n",
      "Best Action sequence:  [1, 3, 3, 1, -2, 1, 3, 1, -2, -3, 1, 1, -4, 3, 3, -4, -2, -3, -1, -2, -2, -2, 3, 1, -4, 3, 1, 1, 3, -2, -2, -4, -3, -4, 3, -3, -4, -3, -1, 1, -4, -3, -4, -2, 4, -2, -4, 2, 1, -4, 3, -3, 3, 3, 3, -3, -2, -2, 1, 4, 4, -4, -2, 3, 3, 3, 1, 2, -1, -4, -4, 4, -4, 3, 3, 4, 1, -1, 1, 4, -3, 3, 3, -2, 4, -4, 3, 1, -3, -4, -1, 1, 4, -1, -4, -4, 1, 3, 4, -4, -2, -2, 2, -1, 0, -3, -1, 1, 4, 3, 3, -4, -4, 1, -1, -4, 4, -1, 2, 4, -2, 4, -1, 1, -3, 0, 1, 3, -4, -1, 4, 2, -2, 1, -2, 2, -2, -1, 4, 0, 3, -3, -1, -2, 1, -2, -4, -1, 3, 2, 4, -4, -2, -2, -3, 2, -1, 4, -4, 1, 2, 1, 1, 1, 4, -4, 3, 2, 4, 2, 4, 2]\n",
      "Action sequence length:  172\n",
      "Best Sum_reward:  141\n",
      "f, x (original):  533.0 [-12.0, 14.0, -13.0, 9.0]\n",
      "f, x (after MCTS):  0.0 [0.0, 2.0, 1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class environment:\n",
    "    def __init__(self):\n",
    "        self.a = 0.\n",
    "        self.b = 2.\n",
    "        self.c = 1.\n",
    "        self.d = 2.\n",
    "        self.x1 = -12.\n",
    "        self.x2 = 14.\n",
    "        self.x3 = -13.\n",
    "        self.x4 = 9.\n",
    "        self.f = self.func()\n",
    "\n",
    "    def func(self):\n",
    "        return (self.x1 - self.a)**2+(self.x2 - self.b)**2+(self.x3 - self.c)**2+(self.x4 - self.d)**2\n",
    "\n",
    "    def step(self, action):\n",
    "        f_before = self.func()\n",
    "\n",
    "        if action == 1:\n",
    "            self.x1 += 1.\n",
    "        elif action == -1:\n",
    "            self.x1 -= 1.\n",
    "        elif action ==  2:\n",
    "            self.x2 += 1.\n",
    "        elif action == -2:\n",
    "            self.x2 -= 1.\n",
    "        elif action ==  3:\n",
    "            self.x3 += 1.\n",
    "        elif action == -3:\n",
    "            self.x3 -= 1.\n",
    "        elif action == 4:\n",
    "            self.x4 += 1.\n",
    "        elif action == -4:\n",
    "            self.x4 -= 1.\n",
    "\n",
    "        f_after = self.func()\n",
    "        if f_after == 0:\n",
    "            reward = 100\n",
    "            terminal = True\n",
    "        else:\n",
    "            reward = 1 if abs(f_before) - abs(f_after) > 0 else -1\n",
    "            terminal = False\n",
    "\n",
    "        return f_after, reward, terminal, [self.x1, self.x2, self.x3, self.x4]\n",
    "\n",
    "    def action_space(self):\n",
    "        return [-4, -3, -2, -1, 0, 1, 2, 3, 4]\n",
    "\n",
    "    def sample(self):\n",
    "        return np.random.choice([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, parent, action):\n",
    "        self.num_visits = 1\n",
    "        self.reward = 0.\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.reward += reward\n",
    "        self.visits += 1\n",
    "\n",
    "    def __repr__(self):\n",
    "        s=\"Reward/Visits =  %.1f/%.1f (Child %d)\"%(self.reward, self.num_visits, len(self.children))\n",
    "        return s\n",
    "\n",
    "def ucb(node):\n",
    "    return node.reward / node.num_visits + sqrt(log(node.parent.num_visits)/node.num_visits)\n",
    "\n",
    "def reward_rate(node):\n",
    "    return node.reward / node.num_visits\n",
    "\n",
    "SHOW_INTERMEDIATE_RESULTS = False\n",
    "\n",
    "env = environment()\n",
    "\n",
    "num_mainloops = 20\n",
    "max_playout_depth = 5\n",
    "num_tree_search = 180\n",
    "\n",
    "best_sum_reward = -inf\n",
    "best_acdtion_sequence = []\n",
    "best_f = 0\n",
    "best_x = []\n",
    "\n",
    "for _ in range(num_mainloops):\n",
    "    root = Node(None, None)\n",
    "\n",
    "\n",
    "    for run_no in range(num_tree_search):\n",
    "        env_copy = copy(env)\n",
    "\n",
    "        terminal = False\n",
    "        sum_reward = 0\n",
    "\n",
    "        # 1) Selection\n",
    "        current_node = root\n",
    "        while len(current_node.children) != 0:\n",
    "                current_node = max(current_node.children, key = ucb)\n",
    "                _, reward, terminal, _ = env_copy.step(current_node.action)\n",
    "                sum_reward += reward\n",
    "\n",
    "        # 2) Expansion\n",
    "        if not terminal:\n",
    "            possible_actions = env_copy.action_space()\n",
    "            current_node.children = [Node(current_node, action) for action in possible_actions]     \n",
    "\n",
    "        # Routine for each children hereafter\n",
    "\n",
    "        for c in current_node.children:\n",
    "            # 3) Playout\n",
    "            env_playout = copy(env_copy)\n",
    "            sum_reward_playout = 0\n",
    "            action_sequence = []\n",
    "\n",
    "            _, reward, terminal, _ = env_playout.step(c.action)\n",
    "            sum_reward_playout += reward\n",
    "            action_sequence.append(c.action)\n",
    "\n",
    "            while not terminal:\n",
    "                action = env_copy.sample()\n",
    "                _, reward, terminal, _ = env_playout.step(action)\n",
    "                sum_reward_playout += reward\n",
    "                action_sequence.append(action)\n",
    "\n",
    "                if len(action_sequence) > max_playout_depth:\n",
    "                    break\n",
    "\n",
    "            if terminal:\n",
    "                print(\"Terminal reached during a playout. #########\")\n",
    "\n",
    "            # 4) Backpropagate\n",
    "            c_ = c\n",
    "            while c_:\n",
    "                c_.num_visits +=1\n",
    "                c_.reward += sum_reward + sum_reward_playout\n",
    "                c_ = c_.parent\n",
    "\n",
    "    #Decision\n",
    "    current_node = root\n",
    "    action_sequence = []\n",
    "    sum_reward = 0\n",
    "    env_copy = copy(env)\n",
    "\n",
    "    while len(current_node.children) != 0:\n",
    "            current_node = max(current_node.children, key = reward_rate)\n",
    "            action_sequence.append(current_node.action)\n",
    "\n",
    "    for action in action_sequence:\n",
    "        _, reward, terminal, _ = env_copy.step(action)\n",
    "        sum_reward += reward\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    f, _, _, x = env_copy.step(0)       \n",
    "\n",
    "    if SHOW_INTERMEDIATE_RESULTS == True:\n",
    "        print(\"Action sequence: \", str(action_sequence))\n",
    "        print(\"Sum_reward: \", str(sum_reward))\n",
    "\n",
    "        print(\"f, x (original): \", env.f , str([env.x1, env.x2]))\n",
    "        print(\"f, x (after MCT): \", str(f), str(x) )\n",
    "        print(\"----------\")\n",
    "\n",
    "    if sum_reward > best_sum_reward:\n",
    "        print(f'updated best: {x}')\n",
    "        best_sum_reward = sum_reward\n",
    "        best_action_sequence = action_sequence\n",
    "        best_f = f\n",
    "        best_x = x\n",
    "\n",
    "print(\"Best Action sequence: \", str(best_action_sequence))\n",
    "print(\"Action sequence length: \", str(len(best_action_sequence)))\n",
    "print(\"Best Sum_reward: \", str(best_sum_reward))\n",
    "print(\"f, x (original): \", env.f , str([env.x1, env.x2, env.x3, env.x4]))\n",
    "print(\"f, x (after MCTS): \", str(best_f), str(best_x) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
