{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques.\n",
    "\n",
    "SOme help from [https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb]\n",
    "\n",
    "### Requires\n",
    "Data sets should be generated by running by this notebook\n",
    "http://localhost:8888/notebooks/sandbox/ipynotebook/UD730/1_notmnist-dataprep.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`; and then reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784), (200000,)\n",
      "Validation set (10000, 784) (10000,)\n",
      "Test set (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  #print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  #print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  #print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "# reshape the image parts 28x28 -> 784\n",
    "(n,width,height) = train_dataset.shape\n",
    "train_dataset =  np.reshape(train_dataset,(n,width*height))[0:n]\n",
    "(n,width,height) = valid_dataset.shape\n",
    "valid_dataset =  np.reshape(valid_dataset,(n,width*height))[0:n]\n",
    "(n,width,height) = test_dataset.shape\n",
    "test_dataset =  np.reshape(test_dataset,(n,width*height))[0:n]\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "trainTF_dataset, trainTF_labels = reformat(train_dataset, train_labels)\n",
    "validTF_dataset, validTF_labels = reformat(valid_dataset, valid_labels)\n",
    "testTF_dataset, testTF_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set %s, %s'  % ( train_dataset.shape, train_labels.shape))\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784), (200000,)\n"
     ]
    }
   ],
   "source": [
    "# this part at the top truncate the data to make it easier to manipulate\n",
    "\n",
    "train_subset = 200000\n",
    "idx = np.random.randint(train_labels.shape[0], size=train_subset)\n",
    "train_dataset = train_dataset[idx, :]\n",
    "train_labels = train_labels[idx]\n",
    "\n",
    "trainTF_dataset, trainTF_labels = reformat(train_dataset, train_labels)\n",
    "validTF_dataset, validTF_labels = reformat(valid_dataset, valid_labels)\n",
    "testTF_dataset, testTF_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set %s, %s'  % ( train_dataset.shape, train_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will be used for scoring normal vectors\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(predictions == labels) / predictions.shape[0])\n",
    "\n",
    "# this will be used for scoring one-hot\n",
    "def accuracyTF(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / labels.shape[0])\n",
    "\n",
    "# placeholders for our Tensor flow input and output\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32) # DROP OUT here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Logistic Network with L2 Regularization\n",
    "\n",
    "Copy from Lesson 1(part2), and add L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 83.69%\t Validation: 81.94%\t Test: 88.97%\n",
      "Elapsed Time: 1622.72\n"
     ]
    }
   ],
   "source": [
    "def SKLogistic():\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_dataset, train_labels)\n",
    "\n",
    "    train_predicted = clf.predict(train_dataset)\n",
    "    valid_predicted = clf.predict(valid_dataset)\n",
    "    test_predicted = clf.predict(test_dataset)\n",
    "    return (accuracy(train_predicted, train_labels), \n",
    "            accuracy(valid_predicted, valid_labels), \n",
    "            accuracy(test_predicted, test_labels))\n",
    "\n",
    "start = time.time()\n",
    "sk_trn_score, sk_vld_score, sk_tst_score = SKLogistic()\n",
    "print(\"Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (sk_trn_score, sk_vld_score, sk_tst_score))\n",
    "print(\"Elapsed Time: %.2f\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Factor 0.000: TFLog Training: 78.17%\t Validation: 76.76%\t Test: 84.65%\n",
      "Elapsed Time: 153.18\n",
      "Reg Factor 0.001: TFLog Training: 81.67%\t Validation: 80.47%\t Test: 87.47%\n",
      "Elapsed Time: 142.55\n",
      "Reg Factor 0.005: TFLog Training: 83.96%\t Validation: 83.19%\t Test: 89.72%\n",
      "Elapsed Time: 45.05\n",
      "Reg Factor 0.010: TFLog Training: 83.76%\t Validation: 82.99%\t Test: 89.75%\n",
      "Elapsed Time: 44.92\n"
     ]
    }
   ],
   "source": [
    "# the TF way - stuff must be one-hoted\n",
    "\n",
    "def TFLogistic(batch_size=128, training_epochs=50, display_step=100, learning_rate=0.01, reg_factor=0.01):\n",
    "\n",
    "    W = tf.Variable(tf.truncated_normal([784, 10]), name=\"weights\")\n",
    "    b = tf.Variable(tf.truncated_normal([10]), name=\"biases\")\n",
    "\n",
    "    logits = tf.matmul(X,W) + b\n",
    "    predict = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "    # for L2 regularization we add something here \n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) +\n",
    "            reg_factor * tf.nn.l2_loss(W))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0.\n",
    "            total_batch = int(trainTF_dataset.shape[0]/batch_size)\n",
    "            for step in range(total_batch):\n",
    "                offset = (step * batch_size) % (trainTF_labels.shape[0] - batch_size)\n",
    "                batch_xs = trainTF_dataset[offset:(offset + batch_size), :]\n",
    "                batch_ys = trainTF_labels[offset:(offset + batch_size)]\n",
    "                _, l, p = sess.run([optimizer, loss, predict], feed_dict={X: batch_xs, y: batch_ys})\n",
    "                avg_loss += l / total_batch # add this rounds portion of the loss avg\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: %04d, loss = %.9f\" % ( (epoch+1), avg_loss))\n",
    "            \n",
    "        trainTF_predicted = sess.run([predict], feed_dict={X: trainTF_dataset})\n",
    "        validTF_predicted = sess.run([predict], feed_dict={X: validTF_dataset})\n",
    "        testTF_predicted = sess.run([predict], feed_dict={X: testTF_dataset})\n",
    "\n",
    "    return (accuracyTF(trainTF_predicted[0], trainTF_labels), \n",
    "            accuracyTF(validTF_predicted[0], validTF_labels), \n",
    "            accuracyTF(testTF_predicted[0], testTF_labels))\n",
    "\n",
    "\n",
    "for reg_factor in (0.0, 0.001, 0.005, 0.01):\n",
    "    start = time.time()\n",
    "    lg_trn_score, lg_vld_score, lg_tst_score = TFLogistic(reg_factor=reg_factor)\n",
    "    print(\"Reg Factor %5.3f: TFLog Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (reg_factor,lg_trn_score, lg_vld_score, lg_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Netowk with L2 Regularization\n",
    "\n",
    "Brining in some of what I learned here [http://localhost:8888/notebooks/sandbox/ipynotebook/tensorflow/NN-MNISTdigits.ipynb] assuming I am running locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Factor 0.000: TFNet Training: 88.17%\t Validation: 83.47%\t Test: 90.04%\n",
      "Elapsed Time: 747.31\n",
      "Reg Factor 0.001: TFNet Training: 83.48%\t Validation: 82.78%\t Test: 89.69%\n",
      "Elapsed Time: 716.05\n",
      "Reg Factor 0.005: TFNet Training: 81.34%\t Validation: 80.99%\t Test: 88.03%\n",
      "Elapsed Time: 716.04\n",
      "Reg Factor 0.010: TFNet Training: 79.27%\t Validation: 78.40%\t Test: 86.00%\n",
      "Elapsed Time: 735.58\n"
     ]
    }
   ],
   "source": [
    "def TFNet(n_hidden1 = 1024, batch_size=128, training_epochs=50, display_step=100, learning_rate=0.01, reg_factor=0.01):\n",
    "    n_input = trainTF_dataset.shape[1]\n",
    "    n_classes = trainTF_labels.shape[1]\n",
    "\n",
    "    # model weights\n",
    "    hl1   = {'weights':tf.Variable(tf.random_normal([n_input,n_hidden1])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_hidden1]))}\n",
    "    outer = {'weights':tf.Variable(tf.random_normal([n_hidden1,n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(X, hl1['weights']), hl1['biases']))\n",
    "    predict = tf.add(tf.matmul(layer1, outer['weights']), outer['biases'])\n",
    "\n",
    "    # with L2 regularization\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict)) +\n",
    "            reg_factor*tf.nn.l2_loss(hl1['weights']) +\n",
    "            reg_factor*tf.nn.l2_loss(outer['weights']))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    n_trainingsize = trainTF_labels.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0.\n",
    "            total_batch = int(trainTF_dataset.shape[0]/batch_size)\n",
    "            for step in range(total_batch):\n",
    "                offset = (step * batch_size) % (n_trainingsize - batch_size)\n",
    "                batch_xs = trainTF_dataset[offset:(offset + batch_size), :]\n",
    "                batch_ys = trainTF_labels[offset:(offset + batch_size)]\n",
    "                _, l = sess.run([optimizer, loss], feed_dict={X: batch_xs, y: batch_ys})\n",
    "                avg_loss += l / total_batch # add this rounds portion of the loss avg\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: %04d, loss = %.9f\" % ( (epoch+1), avg_loss))\n",
    "    \n",
    "        trainTF_predicted = sess.run([predict], feed_dict={X: trainTF_dataset})\n",
    "        validTF_predicted = sess.run([predict], feed_dict={X: validTF_dataset})\n",
    "        testTF_predicted = sess.run([predict], feed_dict={X: testTF_dataset})\n",
    "\n",
    "    return (accuracyTF(trainTF_predicted[0], trainTF_labels), \n",
    "            accuracyTF(validTF_predicted[0], validTF_labels), \n",
    "            accuracyTF(testTF_predicted[0], testTF_labels))\n",
    "\n",
    "\n",
    "for reg_factor in (0.0, 0.001, 0.005, 0.01):\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNet(reg_factor=reg_factor)\n",
    "    print(\"Reg Factor %5.3f: TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (reg_factor,nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size: 200000\n",
      "SKLog Training: 83.76%\t Validation: 82.99%\t Test: 89.75%\n",
      "TFLog Training: 79.27%\t Validation: 78.40%\t Test: 86.00%\n",
      "TFNet Training: 83.69%\t Validation: 81.94%\t Test: 88.97%\n"
     ]
    }
   ],
   "source": [
    "#summarize all scores\n",
    "print(\"Train Data Size: %d\" % train_dataset.shape[0])\n",
    "print(\"SKLog Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (lg_trn_score, lg_vld_score, lg_tst_score))\n",
    "print(\"TFLog Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "print(\"TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (sk_trn_score, sk_vld_score, sk_tst_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 64\n",
      "Size   100: TFNet Training: 14.00%\t Validation: 11.88%\t Test: 11.99%\n",
      "Elapsed Time: 0.79\n",
      "Size   500: TFNet Training: 87.80%\t Validation: 55.00%\t Test: 60.65%\n",
      "Elapsed Time: 2.26\n",
      "Size  1000: TFNet Training: 97.50%\t Validation: 76.11%\t Test: 83.41%\n",
      "Elapsed Time: 4.15\n",
      "Size  2000: TFNet Training: 90.60%\t Validation: 73.36%\t Test: 80.97%\n",
      "Elapsed Time: 7.92\n",
      "Size  3000: TFNet Training: 91.00%\t Validation: 75.53%\t Test: 82.70%\n",
      "Elapsed Time: 11.51\n",
      "Size  4000: TFNet Training: 87.92%\t Validation: 74.93%\t Test: 82.12%\n",
      "Elapsed Time: 15.28\n",
      "Size  5000: TFNet Training: 90.66%\t Validation: 77.38%\t Test: 84.11%\n",
      "Elapsed Time: 19.18\n",
      "\n",
      "batch_size 128\n",
      "Size   100: TFNet Training: 10.00%\t Validation: 10.60%\t Test: 10.62%\n",
      "Elapsed Time: 1.21\n",
      "Size   500: TFNet Training: 90.80%\t Validation: 56.93%\t Test: 63.60%\n",
      "Elapsed Time: 2.70\n",
      "Size  1000: TFNet Training: 96.10%\t Validation: 74.32%\t Test: 81.31%\n",
      "Elapsed Time: 4.52\n",
      "Size  2000: TFNet Training: 94.60%\t Validation: 75.32%\t Test: 83.37%\n",
      "Elapsed Time: 8.27\n",
      "Size  3000: TFNet Training: 93.63%\t Validation: 76.74%\t Test: 83.74%\n",
      "Elapsed Time: 12.18\n",
      "Size  4000: TFNet Training: 87.78%\t Validation: 74.84%\t Test: 83.40%\n",
      "Elapsed Time: 15.90\n",
      "Size  5000: TFNet Training: 89.52%\t Validation: 76.89%\t Test: 84.01%\n",
      "Elapsed Time: 19.64\n",
      "\n",
      "batch size 1024\n",
      "Size  3000: TFNet Training: 89.77%\t Validation: 65.40%\t Test: 73.33%\n",
      "Elapsed Time: 6.46\n",
      "Size  4000: TFNet Training: 92.10%\t Validation: 65.91%\t Test: 73.45%\n",
      "Elapsed Time: 9.27\n",
      "Size  5000: TFNet Training: 95.28%\t Validation: 75.69%\t Test: 83.29%\n",
      "Elapsed Time: 11.83\n",
      "Size  6000: TFNet Training: 95.00%\t Validation: 79.16%\t Test: 86.70%\n",
      "Elapsed Time: 14.49\n",
      "Size  7000: TFNet Training: 95.89%\t Validation: 78.97%\t Test: 86.23%\n",
      "Elapsed Time: 16.47\n",
      "Size  8000: TFNet Training: 93.75%\t Validation: 78.73%\t Test: 85.94%\n",
      "Elapsed Time: 19.11\n",
      "Size  9000: TFNet Training: 89.70%\t Validation: 78.19%\t Test: 84.71%\n",
      "Elapsed Time: 21.51\n",
      "Size 10000: TFNet Training: 92.40%\t Validation: 79.38%\t Test: 86.72%\n",
      "Elapsed Time: 23.90\n"
     ]
    }
   ],
   "source": [
    "# set the original ones aside\n",
    "sTrainTF_dataset = trainTF_dataset\n",
    "sTrainTF_labels = trainTF_labels\n",
    "\n",
    "\n",
    "print(\"batch_size 64\")\n",
    "# better yet take a random selection\n",
    "for subset_size in (100, 500, 1000, 2000, 3000, 4000, 5000):\n",
    "    idx = np.random.randint(sTrainTF_labels.shape[0], size=subset_size)\n",
    "    trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "    trainTF_labels = sTrainTF_labels[idx,:]\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNet(batch_size=128, display_step=1000, reg_factor=0.001)\n",
    "    print(\"Size %5d: TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (subset_size, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "\n",
    "print(\"\")\n",
    "print(\"batch_size 128\")\n",
    "# better yet take a random selection\n",
    "for subset_size in (100, 500, 1000, 2000, 3000, 4000, 5000):\n",
    "    idx = np.random.randint(sTrainTF_labels.shape[0], size=subset_size)\n",
    "    trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "    trainTF_labels = sTrainTF_labels[idx,:]\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNet(batch_size=128, display_step=1000, reg_factor=0.001)\n",
    "    print(\"Size %5d: TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (subset_size, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "\n",
    "print(\"\")\n",
    "print(\"batch size 1024\")\n",
    "for subset_size in (3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000):\n",
    "    idx = np.random.randint(sTrainTF_labels.shape[0], size=subset_size)\n",
    "    trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "    trainTF_labels = sTrainTF_labels[idx,:]\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNet(batch_size=1024, display_step=1000, reg_factor=0.001)\n",
    "    print(\"Size %5d: TFNet Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (subset_size, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# restore the old values\n",
    "trainTF_dataset = sTrainTF_dataset\n",
    "trainTF_labels = sTrainTF_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Factor 0.000: TFNetDropout Training: 85.83%\t Validation: 84.27%\t Test: 90.71%\n",
      "Elapsed Time: 822.81\n",
      "Reg Factor 0.001: TFNetDropout Training: 81.93%\t Validation: 81.30%\t Test: 87.90%\n",
      "Elapsed Time: 835.82\n"
     ]
    }
   ],
   "source": [
    "#  hmmm... how to introduce dropout... I guess google :-)\n",
    "# for now it just does relu instead of sigmoid\n",
    "def TFNetDropout(n_hidden1 = 1024, batch_size=128, training_epochs=50, \n",
    "                 display_step=100, learning_rate=0.01, reg_factor=0.01, keep_factor=.5):\n",
    "    n_input = trainTF_dataset.shape[1]\n",
    "    n_classes = trainTF_labels.shape[1]\n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # model weights\n",
    "    hl1   = {'weights':tf.Variable(tf.random_normal([n_input,n_hidden1])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_hidden1]))}\n",
    "    outer = {'weights':tf.Variable(tf.random_normal([n_hidden1,n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(X, hl1['weights']), hl1['biases']))\n",
    "    drop_out = tf.nn.dropout(layer1, keep_prob ) # apply drop-out to the hidden layer\n",
    "    predict = tf.add(tf.matmul(drop_out, outer['weights']), outer['biases'])\n",
    "\n",
    "    # with L2 regularization\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict)) +\n",
    "            reg_factor*tf.nn.l2_loss(hl1['weights']) +\n",
    "            reg_factor*tf.nn.l2_loss(outer['weights']))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    n_trainingsize = trainTF_labels.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0.\n",
    "            total_batch = int(trainTF_dataset.shape[0]/batch_size)\n",
    "            for step in range(total_batch):\n",
    "                offset = (step * batch_size) % (n_trainingsize - batch_size)\n",
    "                batch_xs = trainTF_dataset[offset:(offset + batch_size), :]\n",
    "                batch_ys = trainTF_labels[offset:(offset + batch_size)]\n",
    "                _, l = sess.run([optimizer, loss], feed_dict={X: batch_xs, y: batch_ys, keep_prob: keep_factor})\n",
    "                avg_loss += l / total_batch # add this rounds portion of the loss avg\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: %04d, loss = %.9f\" % ( (epoch+1), avg_loss))\n",
    "    \n",
    "        trainTF_predicted = sess.run([predict], feed_dict={X: trainTF_dataset, keep_prob:1.0})\n",
    "        validTF_predicted = sess.run([predict], feed_dict={X: validTF_dataset, keep_prob:1.0})\n",
    "        testTF_predicted = sess.run([predict], feed_dict={X: testTF_dataset, keep_prob:1.0})\n",
    "\n",
    "    return (accuracyTF(trainTF_predicted[0], trainTF_labels), \n",
    "            accuracyTF(validTF_predicted[0], validTF_labels), \n",
    "            accuracyTF(testTF_predicted[0], testTF_labels))\n",
    "\n",
    "    \n",
    "for reg_factor in (0.0, 0.001):\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNetDropout(reg_factor=reg_factor)\n",
    "    print(\"Reg Factor %5.3f: TFNetDropout Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (reg_factor, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2000\n",
      "Epoch: 0100, loss = 2.205365785\n",
      "keep 0.50: TFNetDropout Training: 94.90%\t Validation: 79.60%\t Test: 86.60%\n",
      "Elapsed Time: 17.97\n",
      "Epoch: 0100, loss = 1.619275729\n",
      "keep 0.70: TFNetDropout Training: 94.25%\t Validation: 77.85%\t Test: 85.24%\n",
      "Elapsed Time: 18.08\n",
      "Epoch: 0100, loss = 1.236182682\n",
      "keep 0.90: TFNetDropout Training: 91.95%\t Validation: 76.38%\t Test: 83.20%\n",
      "Elapsed Time: 18.22\n",
      "Epoch: 0100, loss = 1.119349917\n",
      "keep 1.00: TFNetDropout Training: 94.20%\t Validation: 75.55%\t Test: 82.57%\n",
      "Elapsed Time: 18.22\n"
     ]
    }
   ],
   "source": [
    "# run some extreme overfitting;\n",
    "# set the original ones aside\n",
    "sTrainTF_dataset = trainTF_dataset\n",
    "sTrainTF_labels = trainTF_labels\n",
    "\n",
    "print(\"batch_size 2000\")\n",
    "idx = np.random.randint(sTrainTF_labels.shape[0], size=2000)\n",
    "trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "trainTF_labels = sTrainTF_labels[idx,:]\n",
    "\n",
    "# better yet take a random selection\n",
    "for keep_factor in (.5, .7, .9, 1.):\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNetDropout(training_epochs=100, reg_factor=0.001, keep_factor=keep_factor)\n",
    "    print(\"keep %.2f: TFNetDropout Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (keep_factor, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "#nn_trn_score, nn_vld_score, nn_tst_score = TFNetDropout(reg_factor=0.001, keep_factor=0.5)\n",
    "#print(\"keep %2f: TFNetDropout Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (keep_factor, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "\n",
    "\n",
    "# restore the old values\n",
    "trainTF_dataset = sTrainTF_dataset\n",
    "trainTF_labels = sTrainTF_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assumes train,test,valid dataset/labels as  trainTF_dataset are all defined\n",
    "\n",
    "def TFNetBest(n_hidden1 = 1024, n_hidden2 = 512, training_epochs=100, batch_size = 128,\n",
    "                 display_step=200, learning_rate=0.001, reg_factor=0.001, keep_factor=.5):\n",
    "    n_input = trainTF_dataset.shape[1]\n",
    "    n_classes = trainTF_labels.shape[1]\n",
    "    \n",
    "    # model weights\n",
    "    hl1   = {'weights':tf.Variable(tf.random_normal([n_input,n_hidden1])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_hidden1]))}\n",
    "    hl2   = {'weights':tf.Variable(tf.random_normal([n_hidden1,n_hidden2])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_hidden2]))}\n",
    "    outer = {'weights':tf.Variable(tf.random_normal([n_hidden2,n_classes])),\n",
    "             'biases':tf.Variable(tf.random_normal([n_classes]))} \n",
    "    #global_step = tf.Variable(0) # count the number of steps taken\n",
    "    #learning_rate \n",
    "    \n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(X, hl1['weights']), hl1['biases']))\n",
    "    drop_out = tf.nn.dropout(layer1, keep_prob ) # apply drop-out to the hidden layer\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(drop_out, hl2['weights']), hl2['biases']))\n",
    "    predict = tf.add(tf.matmul(layer2, outer['weights']), outer['biases'])\n",
    "\n",
    "    # with L2 regularization\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict)) +\n",
    "            reg_factor*tf.nn.l2_loss(hl1['weights']) +\n",
    "            reg_factor*tf.nn.l2_loss(hl2['weights']) +\n",
    "            reg_factor*tf.nn.l2_loss(outer['weights']))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "    n_trainingsize = trainTF_labels.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(training_epochs):\n",
    "            avg_loss = 0.\n",
    "            total_batch = int(trainTF_dataset.shape[0]/batch_size)\n",
    "            for step in range(total_batch):\n",
    "                offset = (step * batch_size) % (n_trainingsize - batch_size)\n",
    "                batch_xs = trainTF_dataset[offset:(offset + batch_size), :]\n",
    "                batch_ys = trainTF_labels[offset:(offset + batch_size)]\n",
    "                _, l = sess.run([optimizer, loss], feed_dict={X: batch_xs, y: batch_ys, keep_prob: keep_factor})\n",
    "                avg_loss += l / total_batch # add this rounds portion of the loss avg\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: %04d, loss = %.9f\" % ( (epoch+1), avg_loss))\n",
    "    \n",
    "        trainTF_predicted = sess.run([predict], feed_dict={X: trainTF_dataset, keep_prob:1.0})\n",
    "        validTF_predicted = sess.run([predict], feed_dict={X: validTF_dataset, keep_prob:1.0})\n",
    "        testTF_predicted = sess.run([predict], feed_dict={X: testTF_dataset, keep_prob:1.0})\n",
    "\n",
    "    return (accuracyTF(trainTF_predicted[0], trainTF_labels), \n",
    "            accuracyTF(validTF_predicted[0], validTF_labels), \n",
    "            accuracyTF(testTF_predicted[0], testTF_labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer2  128: TFNet/Best Training: 86.07%\t Validation: 85.15%\t Test: 91.79%\n",
      "Elapsed Time: 1845.73\n",
      "hidden layer2  512: TFNet/Best Training: 86.07%\t Validation: 84.98%\t Test: 91.86%\n",
      "Elapsed Time: 2532.99\n",
      "hidden layer2 1024: TFNet/Best Training: 85.93%\t Validation: 85.12%\t Test: 91.49%\n",
      "Elapsed Time: 3372.50\n"
     ]
    }
   ],
   "source": [
    "#grab a smaller set to test with\n",
    "\n",
    "#print(\"batch_size 2000\")\n",
    "#idx = np.random.randint(sTrainTF_labels.shape[0], size=2000)\n",
    "#trainTF_dataset = sTrainTF_dataset[idx, :]\n",
    "#trainTF_labels = sTrainTF_labels[idx,:]\n",
    "\n",
    "# better yet take a random selection\n",
    "for n_hidden_layer2 in (128, 512, 1024):\n",
    "    start = time.time()\n",
    "    nn_trn_score, nn_vld_score, nn_tst_score = TFNetBest(n_hidden2=n_hidden_layer2)\n",
    "    print(\"hidden layer2 %4d: TFNet/Best Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (n_hidden_layer2, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "    print(\"Elapsed Time: %.2f\" % (time.time() - start))\n",
    "#nn_trn_score, nn_vld_score, nn_tst_score = TFNetBest(reg_factor=0.001, keep_factor=0.5)\n",
    "#print(\"keep %2f: TFNetDropout Training: %.2f%%\\t Validation: %.2f%%\\t Test: %.2f%%\" % (keep_factor, nn_trn_score, nn_vld_score, nn_tst_score))\n",
    "\n",
    "\n",
    "# restore the old values\n",
    "#trainTF_dataset = sTrainTF_dataset\n",
    "#trainTF_labels = sTrainTF_labels"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
