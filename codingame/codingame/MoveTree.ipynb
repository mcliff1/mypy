{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move Tree\n",
    "notebook will implement a basic tree mode that covers changes in our set ItemSpace which is a wrapper for n-dimensional integer.\n",
    "\n",
    "We wish to construct a tree for a collection of moves (which are represented by itemsets)\n",
    "where each Node has a given state (ItemSet)\n",
    "\n",
    "\n",
    "Ok, this was original work, but then discovered I am essentially making a *Monte Carlo Tree Search* algorithm, so I read the literature. I think I can use [y_yoko](https://qiita.com/y_yoko/items/07b9e3e8d4a43c61d39f) as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSet(object):\n",
    "    \"\"\" \n",
    "    this is the basically space of our game\n",
    "    it can represent an inventory or items, or a play/move to convert from one set to another\n",
    "     or can represent a recipe (target state)\n",
    "    \"\"\"\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "\n",
    "    def is_positive(self):\n",
    "        return all(map(lambda x: x>=0, self.items))    \n",
    "        \n",
    "    def __add__(self, addend): \n",
    "        return ItemSet(list(map(lambda x:x[0]+x[1], zip(self.items, addend.items))))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(map(abs,self.items))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'({self.items})'\n",
    "\n",
    "class State(object):\n",
    "    \"\"\" represents the state of our game - inventory and history \"\"\"\n",
    "    def __init__(self, play_history, inventory):\n",
    "        self.inventory = inventory\n",
    "        self.play_history = play_history\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(str(self.play_history))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'State({self.inventory}, h={self.play_history})'\n",
    "    \n",
    "class Game(object):\n",
    "    \"\"\" represents our stateless game, with available moves \"\"\"\n",
    "    def __init__(self, inventory=None):\n",
    "        self.initial_inventory = inventory if inventory else ItemSet([0,0])\n",
    "    \n",
    "    MOVES = [ItemSet([2,0]), ItemSet([-1,1])]\n",
    "    \n",
    "    def start(self):\n",
    "        return State([], self.initial_inventory)\n",
    "    \n",
    "    def available_moves(self, state):\n",
    "        return [m for m in Game.MOVES if (state.inventory + m).is_positive()]\n",
    "    \n",
    "    def next_state(self, state, move):\n",
    "        #return new_state\n",
    "        new_history = state.play_history.copy()\n",
    "        new_history.append(move)\n",
    "        \n",
    "        return State(new_history, state.inventory + move) \n",
    "    \n",
    "    \n",
    "    def reward(self, state):\n",
    "        \"\"\" reward function for the state \"\"\"\n",
    "        return 100. if self.winner(state) else 0.\n",
    "    \n",
    "    def winner(self, state):\n",
    "        \"\"\" return true if self.state is a winning state \"\"\"\n",
    "        #return winner\n",
    "        if state.inventory.items[1] > 1:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Game(init={self.initial_inventory}, moves={self.MOVES})'    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winner: False: State(([0, 0]), h=[]) -> [([2, 0])]\n",
      "winner: False: State(([2, 0]), h=[([2, 0])]) -> [([2, 0]), ([-1, 1])]\n",
      "winner: False: State(([1, 1]), h=[([2, 0]), ([-1, 1])]) -> [([2, 0]), ([-1, 1])]\n",
      "winner: True: State(([0, 2]), h=[([2, 0]), ([-1, 1]), ([-1, 1])]) -> [([2, 0])]\n",
      "winner: True: State(([2, 2]), h=[([2, 0]), ([-1, 1]), ([-1, 1]), ([2, 0])]) -> [([2, 0]), ([-1, 1])]\n",
      "winner: True: State(([1, 3]), h=[([2, 0]), ([-1, 1]), ([-1, 1]), ([2, 0]), ([-1, 1])]) -> [([2, 0]), ([-1, 1])]\n"
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "state = game.start()\n",
    "moves = game.available_moves(state)\n",
    "print(f'winner: {game.winner(state)}: {state} -> {moves}')\n",
    "state = game.next_state(state, moves[-1])\n",
    "moves = game.available_moves(state)\n",
    "print(f'winner: {game.winner(state)}: {state} -> {moves}')\n",
    "state = game.next_state(state, moves[-1])\n",
    "moves = game.available_moves(state)\n",
    "print(f'winner: {game.winner(state)}: {state} -> {moves}')\n",
    "state = game.next_state(state, moves[-1])\n",
    "moves = game.available_moves(state)\n",
    "print(f'winner: {game.winner(state)}: {state} -> {moves}')\n",
    "state = game.next_state(state, moves[-1])\n",
    "moves = game.available_moves(state)\n",
    "print(f'winner: {game.winner(state)}: {state} -> {moves}')\n",
    "state = game.next_state(state, moves[-1])\n",
    "moves = game.available_moves(state)\n",
    "print(f'winner: {game.winner(state)}: {state} -> {moves}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloTreeNode(object):\n",
    "    def __init__(self, parent, action):\n",
    "        self.num_visits = 1\n",
    "        self.reward = 0.\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.reward += reward\n",
    "        self.visits += 1\n",
    "\n",
    "\n",
    "    def ucb(self):\n",
    "        return self.reward / self.num_visits + math.sqrt(math.log(self.parent.num_visits)/self.num_visits)\n",
    "\n",
    "    def reward_rate(self):\n",
    "        return self.reward / self.num_visits        \n",
    "        \n",
    "    def recurse(self):\n",
    "        c = '(' + ','.join([child.recurse() for child in self.children]) + ')' if self.children else 'None'\n",
    "        return f'Node(a={self.action}, r={self.reward}, c={c})'\n",
    "\n",
    "    def __repr__(self):\n",
    "        #s=\"Reward/Visits =  %.1f/%.1f (Child %d)\"%(self.reward, self.num_visits, len(self.children))\n",
    "        s=\"{Reward/Visits =  %.1f/%.1f, Child %d}\"%(self.reward, self.num_visits, len(self.children))\n",
    "        return f'Node(p={self.parent}, a={self.action}, s={s})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting node: Node(p=None, a=None, s={Reward/Visits =  0.0/1.0, Child 0}) for expansion\n",
      "set current_node.children.action[] None to [([2, 0])]\n",
      "complete run 0: state: State(([0, 0]), h=[])\n",
      "selecting node: Node(p=Node(p=None, a=None, s={Reward/Visits =  0.0/2.0, Child 1}), a=([2, 0]), s={Reward/Visits =  0.0/2.0, Child 0}) for expansion\n",
      "set current_node.children.action[] Node(p=None, a=None, s={Reward/Visits =  0.0/2.0, Child 1}) to [([2, 0]), ([-1, 1])]\n",
      "Terminal State(([2, 2]), h=[([2, 0]), ([-1, 1]), ([2, 0]), ([-1, 1])]) reached during a playout. #########\n",
      "complete run 1: state: State(([2, 0]), h=[([2, 0])])\n",
      "selecting node: Node(p=Node(p=Node(p=None, a=None, s={Reward/Visits =  100.0/4.0, Child 1}), a=([2, 0]), s={Reward/Visits =  100.0/4.0, Child 2}), a=([-1, 1]), s={Reward/Visits =  100.0/2.0, Child 0}) for expansion\n",
      "set current_node.children.action[] Node(p=Node(p=None, a=None, s={Reward/Visits =  100.0/4.0, Child 1}), a=([2, 0]), s={Reward/Visits =  100.0/4.0, Child 2}) to [([2, 0]), ([-1, 1])]\n",
      "Terminal State(([6, 2]), h=[([2, 0]), ([-1, 1]), ([2, 0]), ([2, 0]), ([2, 0]), ([-1, 1])]) reached during a playout. #########\n",
      "Terminal State(([0, 2]), h=[([2, 0]), ([-1, 1]), ([-1, 1])]) reached during a playout. #########\n",
      "complete run 2: state: State(([1, 1]), h=[([2, 0]), ([-1, 1])])\n",
      "make a decision on root: Node(a=None, r=300.0, c=(Node(a=([2, 0]), r=300.0, c=(Node(a=([2, 0]), r=0.0, c=None),Node(a=([-1, 1]), r=300.0, c=(Node(a=([2, 0]), r=100.0, c=None),Node(a=([-1, 1]), r=100.0, c=None)))))))\n",
      "Action sequence:  [([2, 0]), ([-1, 1]), ([2, 0])]\n",
      "Sum_reward:  100.0\n",
      "----------\n",
      "updated best: Node(p=Node(p=Node(p=Node(p=None, a=None, s={Reward/Visits =  300.0/6.0, Child 1}), a=([2, 0]), s={Reward/Visits =  300.0/6.0, Child 2}), a=([-1, 1]), s={Reward/Visits =  300.0/4.0, Child 2}), a=([2, 0]), s={Reward/Visits =  100.0/2.0, Child 0})\n",
      "Best Action sequence:  [([2, 0]), ([-1, 1]), ([2, 0])]\n",
      "Action sequence length:  3\n",
      "Best Sum_reward:  100.0\n"
     ]
    }
   ],
   "source": [
    "#SHOW_INTERMEDIATE_RESULTS = False\n",
    "SHOW_INTERMEDIATE_RESULTS = True\n",
    "\n",
    "_pick_random = lambda x: x[random.randint(0, len(x)-1)]\n",
    "\n",
    "game = Game()\n",
    "initial_state = game.start()\n",
    "\n",
    "num_mainloops = 1\n",
    "max_playout_depth = 3\n",
    "num_tree_search = 3 # 180\n",
    "\n",
    "best_sum_reward = -math.inf\n",
    "best_action_sequence = []\n",
    "# best_f = 0\n",
    "# best_x = []\n",
    "\n",
    "for _ in range(num_mainloops):\n",
    "    root = MonteCarloTreeNode(None, None)\n",
    "\n",
    "\n",
    "    for run_no in range(num_tree_search):\n",
    "        #env_copy = copy(env)\n",
    "        state = copy(initial_state)\n",
    "        \n",
    "        sum_reward = 0\n",
    "\n",
    "        # 1) Selection\n",
    "        current_node = root\n",
    "        while len(current_node.children) != 0:\n",
    "            current_node = max(current_node.children, key=MonteCarloTreeNode.ucb)\n",
    "            #_, reward, terminal, _ = env_copy.step(current_node.action)\n",
    "            state = game.next_state(state, current_node.action)\n",
    "            #sum_reward += reward\n",
    "            sum_reward += game.reward(state)\n",
    "        print(f'selecting node: {current_node} for expansion')\n",
    "            \n",
    "        # 2) Expansion\n",
    "        #if not terminal:\n",
    "        if not game.winner(state):\n",
    "            #possible_actions = env_copy.action_space()\n",
    "            possible_actions = game.available_moves(state)\n",
    "            current_node.children = [MonteCarloTreeNode(current_node, action) for action in possible_actions]     \n",
    "\n",
    "        print(f'set current_node.children.action[] {current_node.parent} to {[c.action for c in current_node.children]}')\n",
    "        # Routine for each children hereafter\n",
    "\n",
    "        for c in current_node.children:\n",
    "            # 3) Playout\n",
    "            #env_playout = copy(env_copy)\n",
    "            sum_reward_playout = 0\n",
    "            action_sequence = []\n",
    "\n",
    "            #_, reward, terminal, _ = env_playout.step(c.action)\n",
    "            child_state = game.next_state(state, c.action)\n",
    "            #sum_reward_playout += reward\n",
    "            sum_reward_playout += game.reward(child_state)\n",
    "            action_sequence.append(c.action)\n",
    "\n",
    "            #while not terminal:\n",
    "            while not game.winner(child_state):\n",
    "                #action = env_copy.sample()\n",
    "                #print(f'avail moves: {game.available_moves(state)}')\n",
    "                action = _pick_random(game.available_moves(state))\n",
    "                #_, reward, terminal, _ = env_playout.step(action)\n",
    "                child_state = game.next_state(child_state, action)\n",
    "                #sum_reward_playout += reward\n",
    "                sum_reward_playout += game.reward(child_state)\n",
    "                action_sequence.append(action)\n",
    "\n",
    "                if len(action_sequence) > max_playout_depth:\n",
    "                    break\n",
    "\n",
    "            if game.winner(child_state):\n",
    "                print(f'Terminal {child_state} reached during a playout. #########')\n",
    "\n",
    "            # 4) Backpropagate\n",
    "            c_ = c\n",
    "            while c_:\n",
    "                c_.num_visits +=1\n",
    "                c_.reward += sum_reward + sum_reward_playout\n",
    "                c_ = c_.parent\n",
    "\n",
    "                \n",
    "        print(f'complete run {run_no}: state: {state}')\n",
    "                \n",
    "    print(f'make a decision on root: {root.recurse()}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Decision\n",
    "    current_node = root\n",
    "    action_sequence = []\n",
    "    sum_reward = 0\n",
    "    #env_copy = copy(env)\n",
    "\n",
    "    while len(current_node.children) != 0:\n",
    "        current_node = max(current_node.children, key=MonteCarloTreeNode.reward_rate)\n",
    "        action_sequence.append(current_node.action)\n",
    "\n",
    "    for action in action_sequence:\n",
    "        #_, reward, terminal, _ = env_copy.step(action)\n",
    "        new_state = game.next_state(state, action)\n",
    "        #sum_reward += reward\n",
    "        sum_reward += game.reward(new_state)\n",
    "        if game.winner(new_state):\n",
    "            break\n",
    "\n",
    "    #f, _, _, x = env_copy.step(0)       \n",
    "\n",
    "    if SHOW_INTERMEDIATE_RESULTS == True:\n",
    "        print(\"Action sequence: \", str(action_sequence))\n",
    "        print(\"Sum_reward: \", str(sum_reward))\n",
    "\n",
    "        #print(\"f, x (original): \", env.f , str([env.x1, env.x2]))\n",
    "        #print(\"f, x (after MCT): \", str(f), str(x) )\n",
    "        print(\"----------\")\n",
    "\n",
    "    if sum_reward > best_sum_reward:\n",
    "        print(f'updated best: {current_node}')\n",
    "        best_sum_reward = sum_reward\n",
    "        best_action_sequence = action_sequence\n",
    "        #best_f = f\n",
    "        #best_x = x\n",
    "\n",
    "print(\"Best Action sequence: \", str(best_action_sequence))\n",
    "print(\"Action sequence length: \", str(len(best_action_sequence)))\n",
    "print(\"Best Sum_reward: \", str(best_sum_reward))\n",
    "#print(\"f, x (original): \", env.f , str([env.x1, env.x2, env.x3, env.x4]))\n",
    "#print(\"f, x (after MCTS): \", str(best_f), str(best_x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-1212e9130a41>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-1212e9130a41>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    Node(None, None: Reward/Visits =  200.0/4.0 (Child 1)), ([2, 0]): Reward/Visits =  200.0/4.0 (Child 2)), ([2, 0]): Reward/Visits =  100.0/2.0 (Child 0)\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# selecting node: \n",
    "# Node(\n",
    "#     p=Node(\n",
    "#         p=Node(p=None, a=None, s={Reward/Visits =  200.0/4.0, Child 1}), \n",
    "#         a=([2, 0]), \n",
    "#         s={Reward/Visits =  200.0/4.0, Child 2}\n",
    "#     ), \n",
    "#     a=([2, 0]), \n",
    "#     s={Reward/Visits =  100.0/2.0, Child 0}\n",
    "# )\n",
    "\n",
    "# Node(\n",
    "#     a=None, \n",
    "#     r=600.0, \n",
    "#     c=(\n",
    "#         Node(\n",
    "#             a=([2, 0]), \n",
    "#             r=600.0, \n",
    "#             c=(\n",
    "#                 Node(\n",
    "#                     a=([2, 0]),\n",
    "#                     r=500.0, \n",
    "#                     c=(\n",
    "#                         Node(\n",
    "#                             a=([2, 0]), \n",
    "#                             r=300.0, \n",
    "#                             c=(\n",
    "#                                 Node(\n",
    "#                                     a=([2, 0]), \n",
    "#                                     r=100.0, \n",
    "#                                     c=None\n",
    "#                                 ),\n",
    "#                                 Node(\n",
    "#                                     a=([-1, 1]), \n",
    "#                                     r=100.0, \n",
    "#                                     c=None\n",
    "#                                 )\n",
    "#                             )\n",
    "#                     ),\n",
    "#                     Node(\n",
    "#                         a=([-1, 1]), \n",
    "#                         r=100.0, \n",
    "#                         c=None\n",
    "#                     )\n",
    "#                 )\n",
    "#             ),\n",
    "#             Node(\n",
    "#                 a=([-1, 1]), \n",
    "#                 r=100.0, \n",
    "#                 c=None\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(([0, 0]), h=[])\n",
      "win: False: State(([2, 0]), h=[([2, 0])])\n",
      "win: False: State(([1, 1]), h=[([2, 0]), ([-1, 1])])\n"
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "state = game.start()\n",
    "print(f'{state}')\n",
    "for action in best_action_sequence:\n",
    "    state = game.next_state(state, action)\n",
    "    print(f'win: {game.winner(state)}: {state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a=None, p=[None], r=100.0] [a=([2, 0]), p=[([2, 0]), None], r=100.0] [a=([2, 0]), p=[([2, 0]), ([2, 0]), None], r=0.0] [a=([-1, 1]), p=[([-1, 1]), ([2, 0]), None], r=100.0] "
     ]
    }
   ],
   "source": [
    "def _parent(node):\n",
    "    \"\"\" returns all actions for the node up the parent chain \"\"\"\n",
    "    if node.parent:\n",
    "        return f'{node.action}, {_parent(node.parent)}'\n",
    "    return 'None'\n",
    "\n",
    "def bfs(tree): # tree has .children\n",
    "    visited = [tree]\n",
    "    queue = [tree]\n",
    "    \n",
    "    while queue:\n",
    "        s = queue.pop(0)\n",
    "        #p = s.parent.action if s.parent else 'None'\n",
    "        p = _parent(s)\n",
    "        print(f'[a={s.action}, p=[{p}], r={s.reward}]', end=' ')\n",
    "        for nbh in s.children:\n",
    "            if nbh not in visited:\n",
    "                visited.append(nbh)\n",
    "                queue.append(nbh)\n",
    "                \n",
    "bfs(root)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
